{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise III:<br> Image Segmentation using CNNs\n",
    "</center></h1>\n",
    "\n",
    "## Short summary\n",
    "In this exercise, we will design a CNN-based **encoder-decoder architecture** to segment rgb images. Image segmentation refers to dividing the image into semantically meaningful regions. For instance, representing each object in the scene with a unique color. The current folder has **three files**: \n",
    "- **configSegmenter.py:** this involves definitions of all parameters and data paths\n",
    "- **utilsSegmenter.py:** includes utility functions required to grab and visualize data \n",
    "- **runSegmenter.ipynb:** contains the script to design, train and test the network \n",
    "\n",
    "Make sure that before running this script, you created an environment and **installed all required libraries** such \n",
    "as keras. The very same environment used in Exercise II can be used here as well.\n",
    "\n",
    "## The data\n",
    "There exists also a subfolder called **data** which contains the traning, validation, and testing data each has both RGB input images together with the corresponding ground truth segmentation images.\n",
    "\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Ex | Exercise 1| A class definition of a CNN model  |\n",
    "| 3 | Loading | Needed | Loading parameters and initializing the model |\n",
    "| 4 | Stats | Needed | Show data distribution |\n",
    "| 5 | Data | Needed | Data augementation |\n",
    "| 6 | Data | Needed | Generating the data batches |\n",
    "| 7 | Debug | Needed | Debugging the data |\n",
    "| 8 | Device | Needed | Selecting CPU/GPU |\n",
    "| 9 | Optimization | Exercise 2 | Selecting an optimization method |\n",
    "| 10 | Training | Exercise 2 | Training the model   |\n",
    "| 11 | Testing | Exercise 2| Testing the  method   | \n",
    "| 13 | Plotting | Information  | View some of test samples |\n",
    "| 13 | Saving | Information  | Saving the model|\n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction of what you have done in the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be effective in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We first start with importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key “backend in file /Users/inuaimi/.matplotlib/matplotlibrc, line 1 ('“backend: TkAgg”')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating network model using gpu 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "import os\n",
    "from configSegmenter import *\n",
    "\n",
    "##etting GPUS before importing tensorflow to make sure it loads on the right one\n",
    "cfg = flying_objects_config()\n",
    "if cfg.GPU >=0:\n",
    "    print(\"creating network model using gpu \" + str(cfg.GPU))\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.GPU)\n",
    "elif cfg.GPU >=-1:\n",
    "    print(\"creating network model using cpu \")\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilsSegmenter import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input,  Conv2DTranspose\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Here, we have the network model class definition. In this class, the most important function is the one called **create_model()**. As defined in the exercises section, your task is to update the network architecture defined in this function such that the network will return the highest accuracy for the given training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " class SegmenterDNNModel():\n",
    "    def __init__(self, num_classes=10, batch_size=32, inputShape=(64,64,3), dropout_prob=0.25):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def conv2d_block(self, input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        inputs = Input(shape=self.inputShape)\n",
    "\n",
    "        down1 = Conv2D(32, (3, 3),padding='same')(inputs)\n",
    "        down1 = Activation('relu')(down1)\n",
    "        down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "        down2 = Conv2D(64, (3, 3), padding='same')(down1_pool)\n",
    "        down2 = Activation('relu')(down2)\n",
    "\n",
    "        \n",
    "        \n",
    "        up1 = UpSampling2D((2, 2))(down2)\n",
    "        up1 = concatenate([down1, up1], axis=3)\n",
    "        up1 = Conv2D(256, (3, 3), padding='same')(up1)\n",
    "        up1 = Activation('relu')(up1)\n",
    "\n",
    "        up2 = Conv2D(256, (3, 3), padding='same')(up1)\n",
    "        up2 = Activation('relu')(up2)\n",
    "\n",
    "        classify = Conv2D(self.num_classes, (1, 1), activation='sigmoid')(up2)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=classify)\n",
    "        model.summary()\n",
    "        return model \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We import the network **hyperparameters** and build a simple cnn by calling the class introduced in the previous step. Please note that to change the hyperparameters, you just need to change the values in the file called **configClassifier.py.**. Do not forget to restart kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 128, 128, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 96) 0           activation[0][0]                 \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 256 221440      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 256 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 256 590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 256 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 3)  771         activation_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 831,683\n",
      "Trainable params: 831,683\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = SegmenterDNNModel(num_classes=cfg.NUM_CLASS, batch_size=cfg.BATCH_SIZE, inputShape=image_shape, dropout_prob=cfg.DROPOUT_PROB)\n",
    "model = modelObj.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We call the utility function **show_statistics** to display the data distribution. This is just for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "##################### Training Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 10817\n",
      "total class number \t 3\n",
      "class circular \t 3626 images\n",
      "class square \t 3488 images\n",
      "class triangle \t 3703 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Validation Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2241\n",
      "total class number \t 3\n",
      "class square \t 783 images\n",
      "class triangle \t 745 images\n",
      "class circular \t 713 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Testing Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2220\n",
      "total class number \t 3\n",
      "class square \t 765 images\n",
      "class triangle \t 733 images\n",
      "class circular \t 722 images\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "#### show how the data looks like\n",
    "show_statistics(cfg.training_data_dir, fineGrained=cfg.fineGrained, title=\" Training Data Statistics \")\n",
    "show_statistics(cfg.validation_data_dir, fineGrained=cfg.fineGrained, title=\" Validation Data Statistics \")\n",
    "show_statistics(cfg.testing_data_dir, fineGrained=cfg.fineGrained, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We **augment** the data by flipping the image horizontally or vertically. As described in the exercises section below, one of your tasks is to update this data augmentation part in order to increase the network efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is being augmented!\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "if cfg.DATA_AUGMENTATION:\n",
    "    print(\"Data is being augmented!\")\n",
    "    aug_parameters = ImageDataGenerator(\n",
    "        # zoom_range=0.2, # randomly zoom into images\n",
    "        # rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        # height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "else:\n",
    "    print(\"Data will not be augmented!\")\n",
    "    aug_parameters = ImageDataGenerator(\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We now create batch generators to get small batches from the entire dataset. There is no need to change these functions as they already return **normalized inputs as batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "use_h5 = False\n",
    "h5_path = \"/Users/inuaimi/sw/git/ArtificialFlyingObjects/data_new_no.hdf5\"\n",
    "\n",
    "\n",
    "if use_h5:\n",
    "    import h5py\n",
    "    h5_file = h5py.File(h5_path, 'r')\n",
    "    nbr_train_data = get_dataset_size_h5(h5_file, 'train_x')\n",
    "    nbr_valid_data = get_dataset_size_h5(h5_file, 'val_x')\n",
    "    nbr_test_data = get_dataset_size_h5(h5_file, 'test_x')\n",
    "    train_batch_generator = generate_segmentation_batches_h5(h5_file, 'train', cfg.BATCH_SIZE)\n",
    "    valid_batch_generator = generate_segmentation_batches_h5(h5_file, 'val', cfg.BATCH_SIZE)\n",
    "    test_batch_generator = generate_segmentation_batches_h5(h5_file, 'test', cfg.BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    nbr_train_data = get_dataset_size(cfg.training_data_dir)\n",
    "    nbr_valid_data = get_dataset_size(cfg.validation_data_dir)\n",
    "    nbr_test_data = get_dataset_size(cfg.testing_data_dir)\n",
    "    train_batch_generator = generate_segmentation_batches(cfg.training_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "    valid_batch_generator = generate_segmentation_batches(cfg.validation_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "    test_batch_generator = generate_segmentation_batches(cfg.testing_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "\n",
    "\n",
    "aug_train_batch_generator = generate_augmented_segmentation_batches(train_batch_generator, aug_parameters)\n",
    "aug_valid_batch_generator = generate_augmented_segmentation_batches(valid_batch_generator, aug_parameters)\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) We can visualize how the data looks like for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 128, 3) (32, 128, 128, 4)\n",
      "x (32, 128, 128, 3) float32 0.0 1.0\n",
      "y (32, 128, 128, 4) float32 0.0 1.0\n",
      "(32, 128, 128, 3) (32, 128, 128, 4)\n",
      "x (32, 128, 128, 3) float32 0.0 1.0\n",
      "y (32, 128, 128, 4) float32 0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BATCH_SIZE': 32,\n",
      " 'CLASSES': ['square', 'triangle', 'circular'],\n",
      " 'DATA_AUGMENTATION': True,\n",
      " 'DEBUG_MODE': True,\n",
      " 'DROPOUT_PROB': 0.5,\n",
      " 'GPU': 1,\n",
      " 'IMAGE_CHANNEL': 3,\n",
      " 'IMAGE_HEIGHT': 128,\n",
      " 'IMAGE_WIDTH': 128,\n",
      " 'LEARNING_RATE': 0.001,\n",
      " 'LR_DECAY_FACTOR': 0.1,\n",
      " 'NUM_CLASS': 3,\n",
      " 'NUM_EPOCHS': 1,\n",
      " 'PRINT_EVERY': 20,\n",
      " 'SAVE_EVERY': 1,\n",
      " 'defaultClasses': ['square', 'triangle', 'circular'],\n",
      " 'fineGrained': False,\n",
      " 'fineGrainedClasses': ['square_red',\n",
      "                        'square_green',\n",
      "                        'square_blue',\n",
      "                        'square_yellow',\n",
      "                        'triangle_red',\n",
      "                        'triangle_green',\n",
      "                        'triangle_blue',\n",
      "                        'triangle_yellow',\n",
      "                        'circular_red',\n",
      "                        'circular_green',\n",
      "                        'circular_blue',\n",
      "                        'circular_yellow'],\n",
      " 'testing_data_dir': '../data/FlyingObjectDataset_10K/testing',\n",
      " 'training_data_dir': '../data/FlyingObjectDataset_10K/training',\n",
      " 'validation_data_dir': '../data/FlyingObjectDataset_10K/validation'}\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG_MODE:\n",
    "    t_x, t_y = next(train_batch_generator)\n",
    "    print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "    print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "    a_x, a_y = next(aug_train_batch_generator)\n",
    "    print('x', a_x.shape, a_x.dtype, a_x.min(), a_x.max())\n",
    "    print('y', a_y.shape, a_y.dtype, a_y.min(), a_y.max())\n",
    "    plot_sample_data_with_groundtruth(a_x, a_y)\n",
    "    pprint.pprint (cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) We select which processing unit to use, either CPU or GPU. In case of having multiple GPUs, we can still select which GPU to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We set the training configuration. As a part of the exercises, this function can also be updated to test different **optimization methods** such as **SGD, ADAM,** etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(cfg.LEARNING_RATE)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer= opt, metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) We can now feed the training and validation data to the network. This will train the network for **some epochs**. Note that the epoch number is also predefined in the file called **configSegmenter.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 128, 3) (32, 128, 128, 4)\n",
      "(32, 128, 128, 3) (32, 128, 128, 4)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [32,128,128,4] vs. [32,128,128,3]\n\t [[node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs (defined at <ipython-input-13-717922733ede>:8) ]] [Op:__inference_train_function_1818]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-717922733ede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_batch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           callbacks=[TensorBoard(log_dir=\"logs/{}_{}\".format(cfg.NUM_EPOCHS,cfg.BATCH_SIZE), write_graph=True, write_images=False,\n\u001b[0;32m----> 8\u001b[0;31m                                  histogram_freq=0)])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [32,128,128,4] vs. [32,128,128,3]\n\t [[node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs (defined at <ipython-input-13-717922733ede>:8) ]] [Op:__inference_train_function_1818]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = aug_train_batch_generator,\n",
    "          epochs=cfg.NUM_EPOCHS,\n",
    "          verbose=1,\n",
    "          steps_per_epoch=(nbr_train_data//cfg.BATCH_SIZE), # total batch number\n",
    "          validation_steps=(nbr_valid_data // cfg.BATCH_SIZE), # total batch number\n",
    "          validation_data=valid_batch_generator,\n",
    "          callbacks=[TensorBoard(log_dir=\"logs/{}_{}\".format(cfg.NUM_EPOCHS,cfg.BATCH_SIZE), write_graph=True, write_images=False,\n",
    "                                 histogram_freq=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) We can test the model with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model\n",
    "test_result = model.evaluate_generator(test_batch_generator,\n",
    "                                       steps=(nbr_test_data//cfg.BATCH_SIZE))\n",
    "test_loss = round(test_result[0], 4)\n",
    "test_acc = round(test_result[1], 4)\n",
    "print(\"Test Loss: \", str(test_loss), \"Test Accuracy: \", str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) We can also show sample segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data, t_segments = next(test_batch_generator)\n",
    "pred_segments = model.predict(t_data, batch_size=cfg.BATCH_SIZE)\n",
    "plot_sample_segmentation_results(t_data, t_segments, pred_segments, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Finally, we can save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('./logs/model_epoch_{}.h5'.format(cfg.NUM_EPOCHS)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do all exercises desribed below. Note that all your source code as well as the log folders must be provided as final results **before May 02, 2019.** \n",
    "\n",
    "\n",
    "#### Exercise 1)\n",
    "Update the network architecture given in the function **create_model** of the class SegmenterDNNModel. \n",
    "\n",
    "**Hint:** You can add more convolution, max pooling layers etc. Batch normalization and dropout are other options to be considered. You can also try applying different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. batchnorm, dropout) methods to increase the network accuracy. Try adding more skip connections from early encoder layers to deeper decoder layers as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "All network resposes are stored in a **log folder** which is automatically created. To visualize these responses, we can use the tensorboard as follows:\n",
    "- First make sure that there is a new folder created with **a date and time stamp** under folder **logs**\n",
    "- Next, open a terminal and type \n",
    "    > tensorboard --logdir=./logs\n",
    "- Finally, open a web browser and type \n",
    "    > http://localhost:6006\n",
    "- You can have an overview of all accuracies on the tensorboard. For more information about tensorboard, please see https://www.tensorflow.org/guide/summaries_and_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
